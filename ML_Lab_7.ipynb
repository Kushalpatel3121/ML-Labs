{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOQwRMD8+Q0YRpGRyRWp0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kushalpatel3121/ML-Labs/blob/main/ML_Lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRQGqU20nHtl"
      },
      "outputs": [],
      "source": [
        "#Importing Libraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the csv file , del 2 cols from the file, checking first few rows of the file.\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "data = pd.read_csv(io.BytesIO(uploaded['BuyComputer.csv']))\n",
        "\n",
        "data.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "EMJfa61Vpc2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Declaring label as the last column of the source file\n",
        "label1 = data['Purchased'].tolist()\n",
        "label = np.array(label1)\n",
        "print(label)"
      ],
      "metadata": {
        "id": "Tm3I-o2Sr4vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Declaring X as all columns excluding last\n",
        "X = data.iloc[:,0:2]\n",
        "print(X)"
      ],
      "metadata": {
        "id": "lOlwjfgVtZMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,label,test_size = 0.4,random_state = 42)\n",
        "# print(X_train)\n",
        "# print(X_test)\n",
        "# print(y_train)\n",
        "# print(y_test)"
      ],
      "metadata": {
        "id": "HjolvNAMw6Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "# print(X_train)\n",
        "# print(X_test)"
      ],
      "metadata": {
        "id": "LYkcw3EvxKRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly training the model\n",
        "\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# classifier = LogisticRegression(random_state = 0)\n",
        "# classifier.fit(X_train,y_train)\n",
        "\n",
        "# y_pred = classifier.predict(X_test)\n",
        "# print(y_pred)\n",
        "\n",
        "# print(\"Accuracy : \",accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "nIt9kSQ13Ivz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variabes to calculate sigmoid function\n",
        "y_pred = []\n",
        "len_x = len(X_train[0])\n",
        "w = []\n",
        "b = 0.2\n",
        "print(len_x)\n",
        "entries = len(X_train[:,0])\n",
        "\n",
        "for weights in range(len_x):\n",
        "    w.append(0)\n",
        "w"
      ],
      "metadata": {
        "id": "LtMnqvC-vDBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fc5297-a4b2-41e5-95bd-7823d116c894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "#Prediction\n",
        "def predict(input):\n",
        "    z = np.dot(input, w) + b\n",
        "    h= sigmoid(z)\n",
        "    for i in range(len(h)):\n",
        "        if(h[i]>=0.5):\n",
        "            h[i]=1\n",
        "        else:\n",
        "            h[i]=0\n",
        "    return h\n",
        "    #Loss function\n",
        "def loss_func(y, y1):\n",
        "    total_bce_loss = np.sum(-y * np.log(y1) - (1 - y) * np.log(1 - y1))\n",
        "    m = y.shape[0]\n",
        "    j = total_bce_loss /m\n",
        "    return j"
      ],
      "metadata": {
        "id": "tTQtHbr51Gne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dw = []\n",
        "db = 0\n",
        "J = 0\n",
        "alpha = 0.1\n",
        "for x in range(len_x):\n",
        "    dw.append(0)"
      ],
      "metadata": {
        "id": "vgCe8CDN76YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeating the process 300 times\n",
        "for i in range(300):\n",
        "    z = np.dot(X_train, w) + b\n",
        "    y_pred = sigmoid(z)\n",
        "    l = loss_func(y_pred, y_train)\n",
        "    dw = np.dot((y_pred-y_train).T, X_train)/X_train.shape[0]\n",
        "    db = np.mean(y_pred-y_train)\n",
        "    w = w - alpha * dw\n",
        "    b = b - alpha* db\n",
        "    print(\"Round:\",i,\"Weight:\",w,\"Bias:\",b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5T3WB_q8Dci",
        "outputId": "313ee635-7ad2-4d41-c04e-27805b96871f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-1622761bf0c7>:16: RuntimeWarning: divide by zero encountered in log\n",
            "  total_bce_loss = np.sum(-y * np.log(y1) - (1 - y) * np.log(1 - y1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round: 0 Weight: [0.03069456 0.01910722] Bias: 0.18234993360208554\n",
            "Round: 1 Weight: [0.06054473 0.03760655] Bias: 0.16513884285260955\n",
            "Round: 2 Weight: [0.08957358 0.05551683] Bias: 0.14835927823399458\n",
            "Round: 3 Weight: [0.11780518 0.07285753] Bias: 0.13200287723885462\n",
            "Round: 4 Weight: [0.1452643  0.08964854] Bias: 0.1160605212741687\n",
            "Round: 5 Weight: [0.17197617 0.10590996] Bias: 0.10052248560409181\n",
            "Round: 6 Weight: [0.1979662  0.12166189] Bias: 0.08537857935391886\n",
            "Round: 7 Weight: [0.22325975 0.13692426] Bias: 0.07061827356975754\n",
            "Round: 8 Weight: [0.24788199 0.15171675] Bias: 0.05623081618455358\n",
            "Round: 9 Weight: [0.27185771 0.16605864] Bias: 0.042205333453886544\n",
            "Round: 10 Weight: [0.29521123 0.17996869] Bias: 0.028530917986796453\n",
            "Round: 11 Weight: [0.31796624 0.19346516] Bias: 0.015196703914389792\n",
            "Round: 12 Weight: [0.3401458  0.20656568] Bias: 0.002191930027629241\n",
            "Round: 13 Weight: [0.36177223 0.21928724] Bias: -0.0104940081043833\n",
            "Round: 14 Weight: [0.38286708 0.23164617] Bias: -0.022871515931805454\n",
            "Round: 15 Weight: [0.40345111 0.24365816] Bias: -0.03495076650776023\n",
            "Round: 16 Weight: [0.42354429 0.25533818] Bias: -0.046741674752204025\n",
            "Round: 17 Weight: [0.44316576 0.26670055] Bias: -0.058253878346484246\n",
            "Round: 18 Weight: [0.46233388 0.27775895] Bias: -0.06949672428686696\n",
            "Round: 19 Weight: [0.48106621 0.28852637] Bias: -0.08047926021458668\n",
            "Round: 20 Weight: [0.49937953 0.29901523] Bias: -0.09121022973365994\n",
            "Round: 21 Weight: [0.51728989 0.3092373 ] Bias: -0.10169807102076638\n",
            "Round: 22 Weight: [0.53481259 0.31920378] Bias: -0.11195091812054084\n",
            "Round: 23 Weight: [0.55196224 0.32892534] Bias: -0.12197660440251755\n",
            "Round: 24 Weight: [0.56875276 0.33841207] Bias: -0.13178266773155495\n",
            "Round: 25 Weight: [0.58519744 0.34767358] Bias: -0.1413763569713712\n",
            "Round: 26 Weight: [0.60130894 0.356719  ] Bias: -0.1507646395008317\n",
            "Round: 27 Weight: [0.61709932 0.36555697] Bias: -0.15995420947516542\n",
            "Round: 28 Weight: [0.63258009 0.37419573] Bias: -0.16895149660985156\n",
            "Round: 29 Weight: [0.64776222 0.38264309] Bias: -0.17776267530412174\n",
            "Round: 30 Weight: [0.66265615 0.39090644] Bias: -0.18639367395451856\n",
            "Round: 31 Weight: [0.67727186 0.39899285] Bias: -0.19485018433739038\n",
            "Round: 32 Weight: [0.69161885 0.40690901] Bias: -0.20313767096321142\n",
            "Round: 33 Weight: [0.7057062  0.41466127] Bias: -0.2112613803257773\n",
            "Round: 34 Weight: [0.71954255 0.42225569] Bias: -0.2192263499861692\n",
            "Round: 35 Weight: [0.73313618 0.42969803] Bias: -0.2270374174453816\n",
            "Round: 36 Weight: [0.74649496 0.43699375] Bias: -0.23469922877108784\n",
            "Round: 37 Weight: [0.75962643 0.44414807] Bias: -0.2422162469535442\n",
            "Round: 38 Weight: [0.7725378  0.45116596] Bias: -0.24959275997342759\n",
            "Round: 39 Weight: [0.78523594 0.45805212] Bias: -0.25683288857073616\n",
            "Round: 40 Weight: [0.79772743 0.46481109] Bias: -0.26394059370899825\n",
            "Round: 41 Weight: [0.81001857 0.47144714] Bias: -0.27091968373313036\n",
            "Round: 42 Weight: [0.82211538 0.47796437] Bias: -0.2777738212225316\n",
            "Round: 43 Weight: [0.83402364 0.4843667 ] Bias: -0.2845065295435483\n",
            "Round: 44 Weight: [0.84574887 0.49065786] Bias: -0.2911211991074057\n",
            "Round: 45 Weight: [0.85729635 0.49684141] Bias: -0.29762109334118897\n",
            "Round: 46 Weight: [0.86867118 0.50292075] Bias: -0.3040093543805539\n",
            "Round: 47 Weight: [0.87987821 0.50889916] Bias: -0.31028900849361857\n",
            "Round: 48 Weight: [0.89092212 0.51477973] Bias: -0.31646297124600836\n",
            "Round: 49 Weight: [0.90180738 0.52056546] Bias: -0.3225340524173299\n",
            "Round: 50 Weight: [0.91253831 0.52625919] Bias: -0.32850496067949225\n",
            "Round: 51 Weight: [0.92311904 0.53186366] Bias: -0.3343783080472993\n",
            "Round: 52 Weight: [0.93355355 0.53738149] Bias: -0.34015661411163933\n",
            "Round: 53 Weight: [0.94384565 0.54281519] Bias: -0.3458423100654205\n",
            "Round: 54 Weight: [0.95399902 0.54816716] Bias: -0.35143774253215876\n",
            "Round: 55 Weight: [0.96401721 0.55343972] Bias: -0.35694517720683927\n",
            "Round: 56 Weight: [0.9739036  0.55863508] Bias: -0.3623668023183526\n",
            "Round: 57 Weight: [0.9836615  0.56375537] Bias: -0.367704731922464\n",
            "Round: 58 Weight: [0.99329404 0.56880264] Bias: -0.37296100903391727\n",
            "Round: 59 Weight: [1.00280428 0.57377885] Bias: -0.37813760860591095\n",
            "Round: 60 Weight: [1.01219516 0.57868589] Bias: -0.3832364403648164\n",
            "Round: 61 Weight: [1.0214695  0.58352558] Bias: -0.3882593515076442\n",
            "Round: 62 Weight: [1.03063004 0.58829967] Bias: -0.3932081292694039\n",
            "Round: 63 Weight: [1.03967942 0.59300984] Bias: -0.398084503367153\n",
            "Round: 64 Weight: [1.04862018 0.59765771] Bias: -0.4028901483271854\n",
            "Round: 65 Weight: [1.05745478 0.60224485] Bias: -0.4076266857014823\n",
            "Round: 66 Weight: [1.0661856  0.60677276] Bias: -0.4122956861792273\n",
            "Round: 67 Weight: [1.07481494 0.61124289] Bias: -0.41689867159888155\n",
            "Round: 68 Weight: [1.08334503 0.61565664] Bias: -0.42143711686602225\n",
            "Round: 69 Weight: [1.091778   0.62001536] Bias: -0.42591245178186743\n",
            "Round: 70 Weight: [1.10011594 0.62432036] Bias: -0.4303260627871425\n",
            "Round: 71 Weight: [1.10836086 0.62857289] Bias: -0.4346792946256919\n",
            "Round: 72 Weight: [1.11651472 0.63277417] Bias: -0.43897345193199566\n",
            "Round: 73 Weight: [1.1245794  0.63692536] Bias: -0.4432098007465248\n",
            "Round: 74 Weight: [1.13255673 0.64102762] Bias: -0.44738956996265017\n",
            "Round: 75 Weight: [1.14044848 0.64508203] Bias: -0.45151395270861705\n",
            "Round: 76 Weight: [1.14825638 0.64908965] Bias: -0.4555841076679013\n",
            "Round: 77 Weight: [1.15598209 0.65305151] Bias: -0.4596011603410819\n",
            "Round: 78 Weight: [1.16362722 0.65696861] Bias: -0.4635662042521905\n",
            "Round: 79 Weight: [1.17119335 0.6608419 ] Bias: -0.46748030210233527\n",
            "Round: 80 Weight: [1.178682   0.66467232] Bias: -0.4713444868732435\n",
            "Round: 81 Weight: [1.18609465 0.66846076] Bias: -0.47515976288321987\n",
            "Round: 82 Weight: [1.19343274 0.67220811] Bias: -0.4789271067978829\n",
            "Round: 83 Weight: [1.20069766 0.6759152 ] Bias: -0.4826474685979108\n",
            "Round: 84 Weight: [1.20789077 0.67958286] Bias: -0.48632177250590714\n",
            "Round: 85 Weight: [1.21501338 0.68321189] Bias: -0.4899509178743823\n",
            "Round: 86 Weight: [1.22206678 0.68680305] Bias: -0.49353578003673715\n",
            "Round: 87 Weight: [1.22905221 0.6903571 ] Bias: -0.49707721112303577\n",
            "Round: 88 Weight: [1.23597088 0.69387477] Bias: -0.5005760408422549\n",
            "Round: 89 Weight: [1.24282398 0.69735675] Bias: -0.5040330772326109\n",
            "Round: 90 Weight: [1.24961266 0.70080373] Bias: -0.5074491073814762\n",
            "Round: 91 Weight: [1.25633802 0.70421639] Bias: -0.5108248981163197\n",
            "Round: 92 Weight: [1.26300116 0.70759536] Bias: -0.5141611966680277\n",
            "Round: 93 Weight: [1.26960314 0.71094128] Bias: -0.5174587313078922\n",
            "Round: 94 Weight: [1.27614499 0.71425476] Bias: -0.5207182119594842\n",
            "Round: 95 Weight: [1.28262771 0.71753638] Bias: -0.5239403307865679\n",
            "Round: 96 Weight: [1.28905229 0.72078674] Bias: -0.5271257627581512\n",
            "Round: 97 Weight: [1.29541968 0.72400638] Bias: -0.53027516619171\n",
            "Round: 98 Weight: [1.30173082 0.72719587] Bias: -0.5333891832755735\n",
            "Round: 99 Weight: [1.3079866  0.73035572] Bias: -0.5364684405714036\n",
            "Round: 100 Weight: [1.31418793 0.73348646] Bias: -0.5395135494976575\n",
            "Round: 101 Weight: [1.32033566 0.7365886 ] Bias: -0.5425251067948753\n",
            "Round: 102 Weight: [1.32643063 0.73966261] Bias: -0.5455036949735934\n",
            "Round: 103 Weight: [1.33247368 0.74270899] Bias: -0.5484498827456438\n",
            "Round: 104 Weight: [1.33846559 0.7457282 ] Bias: -0.5513642254395619\n",
            "Round: 105 Weight: [1.34440717 0.7487207 ] Bias: -0.5542472654007895\n",
            "Round: 106 Weight: [1.35029918 0.75168691] Bias: -0.5570995323773266\n",
            "Round: 107 Weight: [1.35614236 0.75462729] Bias: -0.5599215438914519\n",
            "Round: 108 Weight: [1.36193745 0.75754224] Bias: -0.5627138055981045\n",
            "Round: 109 Weight: [1.36768516 0.76043219] Bias: -0.5654768116304876\n",
            "Round: 110 Weight: [1.37338619 0.76329752] Bias: -0.5682110449334306\n",
            "Round: 111 Weight: [1.37904122 0.76613863] Bias: -0.5709169775850187\n",
            "Round: 112 Weight: [1.38465093 0.76895591] Bias: -0.5735950711069759\n",
            "Round: 113 Weight: [1.39021596 0.77174972] Bias: -0.5762457767642637\n",
            "Round: 114 Weight: [1.39573696 0.77452042] Bias: -0.5788695358543354\n",
            "Round: 115 Weight: [1.40121454 0.77726838] Bias: -0.5814667799864687\n",
            "Round: 116 Weight: [1.40664933 0.77999394] Bias: -0.5840379313515742\n",
            "Round: 117 Weight: [1.41204191 0.78269744] Bias: -0.5865834029828642\n",
            "Round: 118 Weight: [1.41739288 0.7853792 ] Bias: -0.5891035990077449\n",
            "Round: 119 Weight: [1.4227028  0.78803956] Bias: -0.5915989148912816\n",
            "Round: 120 Weight: [1.42797225 0.79067882] Bias: -0.5940697376715667\n",
            "Round: 121 Weight: [1.43320176 0.7932973 ] Bias: -0.5965164461873099\n",
            "Round: 122 Weight: [1.43839187 0.7958953 ] Bias: -0.5989394112979527\n",
            "Round: 123 Weight: [1.44354312 0.79847311] Bias: -0.6013389960965954\n",
            "Round: 124 Weight: [1.44865602 0.80103102] Bias: -0.6037155561160149\n",
            "Round: 125 Weight: [1.45373107 0.80356931] Bias: -0.6060694395280368\n",
            "Round: 126 Weight: [1.45876877 0.80608827] Bias: -0.608400987336514\n",
            "Round: 127 Weight: [1.46376961 0.80858815] Bias: -0.6107105335641545\n",
            "Round: 128 Weight: [1.46873406 0.81106923] Bias: -0.6129984054334287\n",
            "Round: 129 Weight: [1.47366259 0.81353177] Bias: -0.6152649235417778\n",
            "Round: 130 Weight: [1.47855565 0.81597601] Bias: -0.617510402031335\n",
            "Round: 131 Weight: [1.4834137 0.8184022] Bias: -0.6197351487533623\n",
            "Round: 132 Weight: [1.48823718 0.82081059] Bias: -0.6219394654275963\n",
            "Round: 133 Weight: [1.49302651 0.82320142] Bias: -0.6241236477966892\n",
            "Round: 134 Weight: [1.49778213 0.82557492] Bias: -0.6262879857759239\n",
            "Round: 135 Weight: [1.50250444 0.82793131] Bias: -0.6284327635983727\n",
            "Round: 136 Weight: [1.50719385 0.83027082] Bias: -0.6305582599556631\n",
            "Round: 137 Weight: [1.51185077 0.83259368] Bias: -0.6326647481345081\n",
            "Round: 138 Weight: [1.51647558 0.83490008] Bias: -0.6347524961491521\n",
            "Round: 139 Weight: [1.52106867 0.83719026] Bias: -0.6368217668698739\n",
            "Round: 140 Weight: [1.52563043 0.8394644 ] Bias: -0.6388728181476887\n",
            "Round: 141 Weight: [1.53016122 0.84172271] Bias: -0.6409059029353784\n",
            "Round: 142 Weight: [1.5346614 0.8439654] Bias: -0.6429212694049806\n",
            "Round: 143 Weight: [1.53913134 0.84619265] Bias: -0.6449191610618563\n",
            "Round: 144 Weight: [1.54357138 0.84840466] Bias: -0.6468998168554554\n",
            "Round: 145 Weight: [1.54798188 0.85060162] Bias: -0.6488634712868917\n",
            "Round: 146 Weight: [1.55236317 0.8527837 ] Bias: -0.6508103545134365\n",
            "Round: 147 Weight: [1.55671558 0.85495109] Bias: -0.6527406924500344\n",
            "Round: 148 Weight: [1.56103945 0.85710396] Bias: -0.6546547068679419\n",
            "Round: 149 Weight: [1.56533509 0.8592425 ] Bias: -0.6565526154905846\n",
            "Round: 150 Weight: [1.56960283 0.86136686] Bias: -0.6584346320867255\n",
            "Round: 151 Weight: [1.57384297 0.86347721] Bias: -0.6603009665610334\n",
            "Round: 152 Weight: [1.57805582 0.86557373] Bias: -0.6621518250421379\n",
            "Round: 153 Weight: [1.58224168 0.86765657] Bias: -0.6639874099682507\n",
            "Round: 154 Weight: [1.58640084 0.86972588] Bias: -0.6658079201704361\n",
            "Round: 155 Weight: [1.59053361 0.87178184] Bias: -0.6676135509536038\n",
            "Round: 156 Weight: [1.59464025 0.87382457] Bias: -0.6694044941752986\n",
            "Round: 157 Weight: [1.59872106 0.87585425] Bias: -0.6711809383223583\n",
            "Round: 158 Weight: [1.60277632 0.87787101] Bias: -0.6729430685855065\n",
            "Round: 159 Weight: [1.60680628 0.879875  ] Bias: -0.6746910669319469\n",
            "Round: 160 Weight: [1.61081123 0.88186636] Bias: -0.6764251121760217\n",
            "Round: 161 Weight: [1.61479143 0.88384524] Bias: -0.6781453800479952\n",
            "Round: 162 Weight: [1.61874712 0.88581176] Bias: -0.6798520432610209\n",
            "Round: 163 Weight: [1.62267858 0.88776607] Bias: -0.6815452715763495\n",
            "Round: 164 Weight: [1.62658605 0.88970829] Bias: -0.6832252318668306\n",
            "Round: 165 Weight: [1.63046978 0.89163856] Bias: -0.6848920881787631\n",
            "Round: 166 Weight: [1.63433001 0.89355701] Bias: -0.6865460017921423\n",
            "Round: 167 Weight: [1.63816698 0.89546376] Bias: -0.6881871312793545\n",
            "Round: 168 Weight: [1.64198093 0.89735894] Bias: -0.6898156325623657\n",
            "Round: 169 Weight: [1.64577209 0.89924266] Bias: -0.6914316589684497\n",
            "Round: 170 Weight: [1.64954068 0.90111506] Bias: -0.6930353612844999\n",
            "Round: 171 Weight: [1.65328694 0.90297624] Bias: -0.6946268878099668\n",
            "Round: 172 Weight: [1.65701109 0.90482632] Bias: -0.6962063844084623\n",
            "Round: 173 Weight: [1.66071334 0.90666542] Bias: -0.6977739945580705\n",
            "Round: 174 Weight: [1.66439391 0.90849366] Bias: -0.6993298594004035\n",
            "Round: 175 Weight: [1.66805301 0.91031113] Bias: -0.7008741177884376\n",
            "Round: 176 Weight: [1.67169085 0.91211795] Bias: -0.7024069063331678\n",
            "Round: 177 Weight: [1.67530764 0.91391423] Bias: -0.7039283594491133\n",
            "Round: 178 Weight: [1.67890358 0.91570008] Bias: -0.7054386093987071\n",
            "Round: 179 Weight: [1.68247886 0.91747559] Bias: -0.706937786335604\n",
            "Round: 180 Weight: [1.68603369 0.91924087] Bias: -0.7084260183469356\n",
            "Round: 181 Weight: [1.68956827 0.92099602] Bias: -0.7099034314945438\n",
            "Round: 182 Weight: [1.69308277 0.92274114] Bias: -0.7113701498552213\n",
            "Round: 183 Weight: [1.6965774  0.92447633] Bias: -0.7128262955599878\n",
            "Round: 184 Weight: [1.70005233 0.92620168] Bias: -0.7142719888324285\n",
            "Round: 185 Weight: [1.70350775 0.92791729] Bias: -0.7157073480261213\n",
            "Round: 186 Weight: [1.70694384 0.92962326] Bias: -0.7171324896611789\n",
            "Round: 187 Weight: [1.71036079 0.93131967] Bias: -0.7185475284599289\n",
            "Round: 188 Weight: [1.71375875 0.93300661] Bias: -0.7199525773817576\n",
            "Round: 189 Weight: [1.71713792 0.93468418] Bias: -0.7213477476571395\n",
            "Round: 190 Weight: [1.72049845 0.93635246] Bias: -0.7227331488208749\n",
            "Round: 191 Weight: [1.72384052 0.93801155] Bias: -0.7241088887445575\n",
            "Round: 192 Weight: [1.7271643  0.93966151] Bias: -0.7254750736682928\n",
            "Round: 193 Weight: [1.73046994 0.94130245] Bias: -0.726831808231688\n",
            "Round: 194 Weight: [1.73375762 0.94293444] Bias: -0.7281791955041327\n",
            "Round: 195 Weight: [1.73702748 0.94455756] Bias: -0.7295173370143891\n",
            "Round: 196 Weight: [1.74027969 0.9461719 ] Bias: -0.7308463327795114\n",
            "Round: 197 Weight: [1.74351441 0.94777754] Bias: -0.7321662813331107\n",
            "Round: 198 Weight: [1.74673178 0.94937454] Bias: -0.7334772797529845\n",
            "Round: 199 Weight: [1.74993195 0.950963  ] Bias: -0.734779423688126\n",
            "Round: 200 Weight: [1.75311509 0.95254299] Bias: -0.7360728073851301\n",
            "Round: 201 Weight: [1.75628133 0.95411458] Bias: -0.7373575237140132\n",
            "Round: 202 Weight: [1.75943082 0.95567785] Bias: -0.7386336641934591\n",
            "Round: 203 Weight: [1.7625637  0.95723286] Bias: -0.7399013190155097\n",
            "Round: 204 Weight: [1.76568012 0.95877971] Bias: -0.7411605770697115\n",
            "Round: 205 Weight: [1.76878022 0.96031844] Bias: -0.7424115259667343\n",
            "Round: 206 Weight: [1.77186413 0.96184914] Bias: -0.7436542520614743\n",
            "Round: 207 Weight: [1.77493199 0.96337187] Bias: -0.7448888404756553\n",
            "Round: 208 Weight: [1.77798393 0.96488671] Bias: -0.746115375119941\n",
            "Round: 209 Weight: [1.7810201  0.96639371] Bias: -0.7473339387155702\n",
            "Round: 210 Weight: [1.78404061 0.96789295] Bias: -0.7485446128155276\n",
            "Round: 211 Weight: [1.7870456 0.9693845] Bias: -0.7497474778252619\n",
            "Round: 212 Weight: [1.7900352  0.97086841] Bias: -0.750942613022961\n",
            "Round: 213 Weight: [1.79300953 0.97234475] Bias: -0.7521300965793986\n",
            "Round: 214 Weight: [1.79596873 0.97381359] Bias: -0.753310005577359\n",
            "Round: 215 Weight: [1.7989129  0.97527499] Bias: -0.7544824160306542\n",
            "Round: 216 Weight: [1.80184218 0.976729  ] Bias: -0.7556474029027402\n",
            "Round: 217 Weight: [1.80475668 0.97817569] Bias: -0.7568050401249447\n",
            "Round: 218 Weight: [1.80765653 0.97961513] Bias: -0.7579554006143143\n",
            "Round: 219 Weight: [1.81054184 0.98104736] Bias: -0.759098556291092\n",
            "Round: 220 Weight: [1.81341273 0.98247245] Bias: -0.7602345780958316\n",
            "Round: 221 Weight: [1.81626931 0.98389046] Bias: -0.7613635360061604\n",
            "Round: 222 Weight: [1.8191117  0.98530144] Bias: -0.762485499053197\n",
            "Round: 223 Weight: [1.82194    0.98670545] Bias: -0.7636005353376333\n",
            "Round: 224 Weight: [1.82475433 0.98810255] Bias: -0.7647087120454883\n",
            "Round: 225 Weight: [1.8275548  0.98949278] Bias: -0.7658100954635415\n",
            "Round: 226 Weight: [1.83034152 0.99087621] Bias: -0.7669047509944544\n",
            "Round: 227 Weight: [1.83311459 0.99225289] Bias: -0.7679927431715861\n",
            "Round: 228 Weight: [1.83587412 0.99362287] Bias: -0.7690741356735113\n",
            "Round: 229 Weight: [1.83862022 0.99498621] Bias: -0.7701489913382478\n",
            "Round: 230 Weight: [1.84135298 0.99634295] Bias: -0.7712173721771987\n",
            "Round: 231 Weight: [1.84407251 0.99769315] Bias: -0.7722793393888184\n",
            "Round: 232 Weight: [1.84677891 0.99903685] Bias: -0.7733349533720072\n",
            "Round: 233 Weight: [1.84947228 1.00037412] Bias: -0.7743842737392405\n",
            "Round: 234 Weight: [1.85215272 1.00170499] Bias: -0.7754273593294402\n",
            "Round: 235 Weight: [1.85482032 1.00302951] Bias: -0.7764642682205932\n",
            "Round: 236 Weight: [1.85747519 1.00434774] Bias: -0.7774950577421234\n",
            "Round: 237 Weight: [1.86011742 1.00565973] Bias: -0.7785197844870212\n",
            "Round: 238 Weight: [1.86274709 1.00696551] Bias: -0.7795385043237393\n",
            "Round: 239 Weight: [1.86536432 1.00826514] Bias: -0.7805512724078563\n",
            "Round: 240 Weight: [1.86796918 1.00955865] Bias: -0.7815581431935164\n",
            "Round: 241 Weight: [1.87056177 1.01084611] Bias: -0.7825591704446484\n",
            "Round: 242 Weight: [1.87314218 1.01212755] Bias: -0.78355440724597\n",
            "Round: 243 Weight: [1.87571049 1.01340302] Bias: -0.7845439060137814\n",
            "Round: 244 Weight: [1.87826681 1.01467255] Bias: -0.7855277185065537\n",
            "Round: 245 Weight: [1.8808112  1.01593621] Bias: -0.7865058958353157\n",
            "Round: 246 Weight: [1.88334377 1.01719402] Bias: -0.7874784884738454\n",
            "Round: 247 Weight: [1.8858646  1.01844602] Bias: -0.7884455462686675\n",
            "Round: 248 Weight: [1.88837376 1.01969228] Bias: -0.7894071184488644\n",
            "Round: 249 Weight: [1.89087135 1.02093281] Bias: -0.7903632536357027\n",
            "Round: 250 Weight: [1.89335745 1.02216767] Bias: -0.79131399985208\n",
            "Round: 251 Weight: [1.89583213 1.0233969 ] Bias: -0.7922594045317956\n",
            "Round: 252 Weight: [1.89829549 1.02462053] Bias: -0.7931995145286496\n",
            "Round: 253 Weight: [1.90074759 1.02583861] Bias: -0.7941343761253732\n",
            "Round: 254 Weight: [1.90318853 1.02705117] Bias: -0.7950640350423943\n",
            "Round: 255 Weight: [1.90561837 1.02825826] Bias: -0.7959885364464423\n",
            "Round: 256 Weight: [1.90803719 1.02945991] Bias: -0.7969079249589947\n",
            "Round: 257 Weight: [1.91044508 1.03065616] Bias: -0.7978222446645697\n",
            "Round: 258 Weight: [1.91284211 1.03184705] Bias: -0.7987315391188681\n",
            "Round: 259 Weight: [1.91522835 1.03303262] Bias: -0.7996358513567661\n",
            "Round: 260 Weight: [1.91760388 1.0342129 ] Bias: -0.8005352239001648\n",
            "Round: 261 Weight: [1.91996878 1.03538793] Bias: -0.8014296987656976\n",
            "Round: 262 Weight: [1.9223231  1.03655774] Bias: -0.8023193174722981\n",
            "Round: 263 Weight: [1.92466694 1.03772238] Bias: -0.8032041210486337\n",
            "Round: 264 Weight: [1.92700035 1.03888187] Bias: -0.8040841500404051\n",
            "Round: 265 Weight: [1.92932342 1.04003626] Bias: -0.8049594445175163\n",
            "Round: 266 Weight: [1.9316362  1.04118557] Bias: -0.8058300440811168\n",
            "Round: 267 Weight: [1.93393877 1.04232984] Bias: -0.806695987870519\n",
            "Round: 268 Weight: [1.9362312  1.04346911] Bias: -0.8075573145699939\n",
            "Round: 269 Weight: [1.93851356 1.04460341] Bias: -0.8084140624154466\n",
            "Round: 270 Weight: [1.94078591 1.04573277] Bias: -0.8092662692009742\n",
            "Round: 271 Weight: [1.94304833 1.04685723] Bias: -0.8101139722853102\n",
            "Round: 272 Weight: [1.94530086 1.04797681] Bias: -0.8109572085981547\n",
            "Round: 273 Weight: [1.94754359 1.04909156] Bias: -0.8117960146463955\n",
            "Round: 274 Weight: [1.94977658 1.05020149] Bias: -0.8126304265202211\n",
            "Round: 275 Weight: [1.95199989 1.05130665] Bias: -0.8134604798991274\n",
            "Round: 276 Weight: [1.95421358 1.05240707] Bias: -0.8142862100578214\n",
            "Round: 277 Weight: [1.95641772 1.05350277] Bias: -0.8151076518720228\n",
            "Round: 278 Weight: [1.95861237 1.05459379] Bias: -0.8159248398241661\n",
            "Round: 279 Weight: [1.96079759 1.05568015] Bias: -0.8167378080090055\n",
            "Round: 280 Weight: [1.96297344 1.0567619 ] Bias: -0.8175465901391242\n",
            "Round: 281 Weight: [1.96513999 1.05783905] Bias: -0.8183512195503493\n",
            "Round: 282 Weight: [1.96729729 1.05891164] Bias: -0.8191517292070754\n",
            "Round: 283 Weight: [1.9694454  1.05997969] Bias: -0.8199481517074982\n",
            "Round: 284 Weight: [1.97158438 1.06104324] Bias: -0.8207405192887601\n",
            "Round: 285 Weight: [1.9737143  1.06210232] Bias: -0.8215288638320082\n",
            "Round: 286 Weight: [1.9758352  1.06315695] Bias: -0.8223132168673684\n",
            "Round: 287 Weight: [1.97794714 1.06420716] Bias: -0.823093609578836\n",
            "Round: 288 Weight: [1.98005019 1.06525297] Bias: -0.8238700728090851\n",
            "Round: 289 Weight: [1.98214439 1.06629443] Bias: -0.8246426370641973\n",
            "Round: 290 Weight: [1.98422981 1.06733155] Bias: -0.8254113325183124\n",
            "Round: 291 Weight: [1.9863065  1.06836435] Bias: -0.8261761890182026\n",
            "Round: 292 Weight: [1.98837451 1.06939288] Bias: -0.8269372360877701\n",
            "Round: 293 Weight: [1.99043389 1.07041715] Bias: -0.8276945029324722\n",
            "Round: 294 Weight: [1.99248471 1.0714372 ] Bias: -0.8284480184436722\n",
            "Round: 295 Weight: [1.99452701 1.07245303] Bias: -0.8291978112029209\n",
            "Round: 296 Weight: [1.99656085 1.0734647 ] Bias: -0.8299439094861665\n",
            "Round: 297 Weight: [1.99858627 1.07447221] Bias: -0.830686341267897\n",
            "Round: 298 Weight: [2.00060334 1.07547559] Bias: -0.8314251342252155\n",
            "Round: 299 Weight: [2.0026121  1.07647488] Bias: -0.832160315741849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the label\n",
        "y_pred=predict(X_train)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXGT-pka8KG_",
        "outputId": "038cbd9d-c7e5-4927-d1d8-26d1fc146547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1.\n",
            " 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print actual and predicted values in a table\n",
        "print(\"actual Value\\tpredicted values\")\n",
        "for i in range(len(y_test)):\n",
        "    print(y_test[i],\"\\t\\t\",int(y_pred[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbo3O59j8NgZ",
        "outputId": "8222871d-f4a0-4eeb-82ea-5257ddf75f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual Value\tpredicted values\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "1 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy of prediction\n",
        "acc = np.sum([y_test[i] == (y_pred[i]) for i in range(len(y_test))])/len(y_test)\n",
        "print(\"Accuracy:\",acc*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1NbLaYi8Q3c",
        "outputId": "c0419971-a129-4455-f013-d4f0a1bb0e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1\n",
        "Using sklearn LogisticRegression model"
      ],
      "metadata": {
        "id": "dNlMGOMi8ZBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(X_train, y_train)\n",
        "a = logisticRegr.predict(X_test[0].reshape(1,-1))\n",
        "a\n",
        "predictions = logisticRegr.predict(X_test)\n",
        "print(predictions)\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxipB8pw8lx8",
        "outputId": "2c451c0f-106f-4d8c-ed42-178e20707c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0]\n",
            "[0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use score method to get accuracy of model\n",
        "score = logisticRegr.score(X_test, y_test)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR9eHZOj84hu",
        "outputId": "ea4f8e50-d3ab-44c6-fabb-42de09e77ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class_names = [0,1]\n",
        "fig,ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks,class_names)\n",
        "plt.yticks(tick_marks,class_names)\n",
        "\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix),annot=True, cmap=\"YlGnBu\",fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('confusion matrix',y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "UqMqlF3P1TGc",
        "outputId": "5147afc1-e7a1-4551-a420-4bf093a993c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 257.44, 'Predicted label')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# target_names = ['without diabetes', 'with diabetes']\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nonrOELx1Zb8",
        "outputId": "6deea6ee-3970-4247-b3e9-f16f0743aeed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      1.00      0.79        78\n",
            "           1       0.00      0.00      0.00        42\n",
            "\n",
            "    accuracy                           0.65       120\n",
            "   macro avg       0.33      0.50      0.39       120\n",
            "weighted avg       0.42      0.65      0.51       120\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "iDU60VGJ1fb3",
        "outputId": "45a6fa31-bc33-46e4-daa3-500746be3a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "import math\n",
        "# reading the csv file, del 2 columns from the file, checking first few rows of the file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/datasets/BuyComputer.csv\")\n",
        "dataset.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "label = dataset.iloc[:,-1].values\n",
        "X = dataset.drop(\"Purchased\" ,axis= 1)\n",
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,\n",
        "label, test_size = 0.30, random_state = 28)\n",
        "# Sacaling data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_Train = sc.fit_transform(X_Train)\n",
        "X_Test = sc.transform(X_Test)\n",
        "\n",
        "# print(X_Train)\n",
        "X_Train=torch.from_numpy(X_Train)\n",
        "y_Train=torch.from_numpy(y_Train)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression(random_state = 28)\n",
        "logisticRegr.fit(X_Train, y_Train)\n",
        "predictions = logisticRegr.predict(X_Test)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "nwZSGlMO7Vyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366f1a4a-f33b-4474-a55a-8a47840e13df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0]\n"
          ]
        }
      ]
    }
  ]
}